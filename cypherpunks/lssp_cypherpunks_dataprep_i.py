# -*- coding: utf-8 -*-
"""LSSP_cypherpunks_dataprep_I.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/d-vf/LSSP-crypto-communities/blob/main/cypherpunks/LSSP_cypherpunks_dataprep_I.ipynb

## Drive

Data source: https://github.com/cryptoanarchywiki

* 1992-2000: https://mailing-list-archive.cryptoanarchy.wiki/ / https://cypherpunks.venona.com/raw/ (228 MB| 98807 mails)
* 2000-2016: https://github.com/cryptoanarchywiki/2000-to-2016-raw-cypherpunks-archive (246,7 MB | /92193 mails)
* July 2013 to Present/March 2023: https://lists.cpunks.org/pipermail/cypherpunks/ (1206 MB | 65795 mails)
"""

from google.colab import drive
drive.mount('/content/drive')

#output
import os
os.chdir('/content/drive/MyDrive/Coursework/88-718/Data')

"""## Libraries"""

! pip install mailbox pendulum dateparser

import numpy as np
import pandas as pd
import os
import sys
import mimetypes
import email
import csv
import re

# Load dataset
data92=pd.read_fwf('/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1992.txt')
#data93=pd.read_fwf('/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1993.txt')
#data94=pd.read_fwf('/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1994.txt')
#data95=pd.read_fwf('/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1995.txt')

# Read in the text file containing the thread of emails
with open( '/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1992.txt', 'r', encoding='latin1')as f:
  text92 = f.read()

text92

# Read in the text file containing the thread of emails
with open('/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/cyp-1998.txt', 'r', encoding='latin1') as f:
    text98 = f.read()

# Split the text into individual email messages
messages = text98.split('\nFrom ')

# Create an empty list to store the email DataFrames
dfs = []

# Loop over each email message and parse it using the email module
for message in messages:
    # Prepend 'From ' to the message to make it a valid email message
    message = 'From ' + message
    
    try:
        # Parse the message using the email module
        msg = email.message_from_string(message)

        # Extract the relevant headers and body
        from_address = msg['From']
        to_address = msg['To']
        message_id = msg['Message-ID']
        date = msg['Date']
        subject = msg['Subject']
        in_reply_to = msg['In-Reply-To']
        mime_version = msg['MIME-Version']
        content_type = msg['Content-Type']
        body = msg.get_payload()

        # Check if the 'From' header is missing
        if from_address is None:
            # If the 'From' header is missing, skip over this message and continue to the next one
            continue

        # Create a new DataFrame for the email message
        df = pd.DataFrame({
            'From': [from_address],
            'To': [to_address],
            'Message-ID': [message_id],
            'Date': [date],
            'Subject': [subject],
            'In-Reply-To': [in_reply_to],
            'MIME-Version': [mime_version],
            'Content-Type': [content_type],
            'Body': [body]
        })

        # Append the DataFrame to the list of DataFrames
        dfs.append(df)

    except Exception as e:
        # If an exception occurs, print the error message and continue processing
        print(f"Error parsing message: {e}")
        continue

# Concatenate the list of DataFrames into a single DataFrame
email_df98 = pd.concat(dfs, ignore_index=True)

email_df98[email_df98['From'].isna()]

"""## cypherpunks

https://mailing-list-archive.cryptoanarchy.wiki/archive/1993/08/4721cc957db8e31b193cbed617f4d5c8de4e5e1a71df76b1d80deb0774be7a1b/

## 90's
"""

import os

# Specify the directory containing the text files
directory = '/content/drive/MyDrive/Coursework/88-718/Data/cryptoanarchy.wiki - Mailing List Archive (1992-2000)/'

# Create an empty list to store the email DataFrames
dfs = []

# Loop over each file in the directory
for filename in os.listdir(directory):
    if filename.endswith('.txt'):  # Only read in text files
        # Read in the text file
        with open(os.path.join(directory, filename), 'r',encoding='ISO-8859-1') as f:
            text = f.read()

        # Split the text into individual email messages
        messages = text.split('\nFrom ')

        # Loop over each email message and parse it using the email module
        for message in messages:
            # Prepend 'From ' to the message to make it a valid email message
            message = 'From ' + message

            try:
                # Parse the message using the email module
                msg = email.message_from_string(message)

                # Extract the relevant headers and body
                from_address = msg['From']
                to_address = msg['To']
                message_id = msg['Message-ID']
                date = msg['Date']
                subject = msg['Subject']
                in_reply_to = msg['In-Reply-To']
                mime_version = msg['MIME-Version']
                content_type = msg['Content-Type']
                body = msg.get_payload()

                # Check if any of the headers are missing
                if from_address is None or to_address is None or message_id is None or date is None:
                    # If any of the headers are missing, skip over this message and continue to the next one
                    continue

                # Create a new DataFrame for the email message
                df = pd.DataFrame({
                    'From': [from_address],
                    'To': [to_address],
                    'Message-ID': [message_id],
                    'Date': [date],
                    'Subject': [subject],
                    'In-Reply-To': [in_reply_to],
                    'MIME-Version': [mime_version],
                    'Content-Type': [content_type],
                    'Body': [body]
                })

                # Append the DataFrame to the list of DataFrames
                dfs.append(df)

            except Exception as e:
                # If an exception occurs, print the error message and continue processing
                print(f"Error parsing message: {e}")
                continue

# Concatenate the list of DataFrames into a single DataFrame
email_df_90 = pd.concat(dfs, ignore_index=True)

email_df_90

email_df_90[email_df_90['From'].isna()]

email_df_90.to_csv('email_df_90.csv', index=False, escapechar='|')

"""## 2000's

https://github.com/cryptoanarchywiki/2000-to-2016-raw-cypherpunks-archive
https://cypherpunks.venona.com/raw/
"""

#breaking into chunks (taking more than 10 just to build path lists)
directory = '/content/drive/MyDrive/Coursework/88-718/Data/2000-to-2016-raw-cypherpunks-archive-master/cur/'
filenames = [os.path.join(directory, filename) for filename in os.listdir(directory)
                 if os.path.isfile(os.path.join(directory, filename))]

len(filenames)

import os

# Specify the directory containing the text files
directory = directory

# Create an empty list to store the email DataFrames
dfs = []

# Loop over each file in the directory
for filename in filenames:
    with open(filename, 'r', encoding='ISO-8859-1') as f:
        # Read in the text file
        text = f.read()

        # Split the text into individual email messages
        messages = text.split('\nFrom ')

        # Loop over each email message and parse it using the email module
        for message in messages:
            # Prepend 'From ' to the message to make it a valid email message
            message = 'From ' + message

            # Parse the message using the email module
            msg = email.message_from_string(message)

        # Extract the relevant headers and body
            from_address = msg['From']
            to_address = msg['To']
            message_id = msg['Message-ID']
            date = msg['Date']
            subject = msg['Subject']
            in_reply_to = msg['In-Reply-To']
            mime_version = msg['MIME-Version']
            content_type = msg['Content-Type']
            body = msg.get_payload()

                    # Create a new DataFrame for the email message
            df = pd.DataFrame({
                'From': [from_address],
                'To': [to_address],
                'Message-ID': [message_id],
                'Date': [date],
                'Subject': [subject],
                'In-Reply-To': [in_reply_to],
                'MIME-Version': [mime_version],
                'Content-Type': [content_type],
                'Body': [body]
                })           

            # Append the DataFrame to the list of DataFrames
            dfs.append(df)

# Concatenate the list of DataFrames into a single DataFrame
email_df_2000 = pd.concat(dfs, ignore_index=True)

import os
import email
import pandas as pd

# Specify the directory containing the text files
directory = '/Users/dianavieirafernandes/Desktop/LSSP-project/cypherpunks/2000-to-2016-raw-cypherpunks-archive-master/cur/'

# Create a list of the filenames in the directory
filenames = [os.path.join(directory, filename) for filename in os.listdir(directory)
             if os.path.isfile(os.path.join(directory, filename))]

# Create an empty list to store the email DataFrames
dfs = []

# Loop over each file in the directory
for filename in filenames:
    try:
        with open(filename, 'r', encoding='ISO-8859-1') as f:
            # Read in the text file
            text = f.read()

            # Split the text into individual email messages
            messages = text.split('\nFrom ')

            # Loop over each email message and parse it using the email module
            for message in messages:
                # Prepend 'From ' to the message to make it a valid email message
                message = 'From ' + message

                # Parse the message using the email module
                msg = email.message_from_string(message)

                # Extract the relevant headers and body
                from_address = msg['From']
                to_address = msg['To']
                message_id = msg['Message-ID']
                date = msg['Date']
                subject = msg['Subject']
                in_reply_to = msg['In-Reply-To']
                mime_version = msg['MIME-Version']
                content_type = msg['Content-Type']
                body = msg.get_payload()

                # Create a new DataFrame for the email message
                df = pd.DataFrame({
                    'From': [from_address],
                    'To': [to_address],
                    'Message-ID': [message_id],
                    'Date': [date],
                    'Subject': [subject],
                    'In-Reply-To': [in_reply_to],
                    'MIME-Version': [mime_version],
                    'Content-Type': [content_type],
                    'Body': [body]
                })

                # Append the DataFrame to the list of DataFrames
                dfs.append(df)

    except Exception as e:
        # If an exception occurs, print the error message and continue processing
        print(f"Error parsing message: {e}")
        continue

# Concatenate the list of DataFrames into a single DataFrame
email_df_2000 = pd.concat(dfs, ignore_index=True)

#email_df2000.to_csv('email_df2000.csv',index=False)
# Write the DataFrame to a CSV file
email_df_2000.to_csv('/Users/dianavieirafernandes/Desktop/LSSP-project/cypherpunks/email_df.csv', index=False)

# Write the DataFrame to a JSON file
email_df_2000.to_json('/Users/dianavieirafernandes/Desktop/LSSP-project/cypherpunks/email_df.json')

#read (as does not run in colab)

"""## 2010-today"""

import mailbox
import pandas as pd

# Specify the path to the mbox file
mbox_file = '/content/drive/MyDrive/Coursework/88-718/Data/cypherpunks.mbox'

# Create an empty list to store the email DataFrames
dfs = []

# Open the mbox file and loop over each email message
mbox = mailbox.mbox(mbox_file)
for msg in mbox:
    # Extract the relevant headers and body

# Extract the relevant headers and body
    from_address = msg['From']
    to_address = msg['To']
    message_id = msg['Message-ID']
    date = msg['Date']
    subject = msg['Subject']
    in_reply_to = msg['In-Reply-To']
    mime_version = msg['MIME-Version']
    content_type = msg['Content-Type']
    body = msg.get_payload()

        # Create a new DataFrame for the email message
    df = pd.DataFrame({
            'From': [from_address],
            'To': [to_address],
            'Message-ID': [message_id],
            'Date': [date],
            'Subject': [subject],
            'In-Reply-To': [in_reply_to],
            'MIME-Version': [mime_version],
            'Content-Type': [content_type],
            'Body': [body]
            }) 
    
    # Append the DataFrame to the list of DataFrames
    dfs.append(df)

# Close the mbox object manually
mbox.close()

# Concatenate the list of DataFrames into a single DataFrame
email_df_2023 = pd.concat(dfs, ignore_index=True)

email_df_2023

email_df_2023.to_csv('email_df_2023.csv',index=False)

email_df_2023[email_df_2023['From'].isna()]

"""## Just read 
(no need to run again)
"""

email_df_90 =pd.read_csv('/content/drive/MyDrive/Coursework/88-718/Data/email_df_90.csv')
email_df_90

email_df_90[email_df_90['From'].isna()]

email_df_90.iloc[[59, 60, 61]]['Body']

email_df_90.dropna(subset=['Body','From'], inplace=True)

email_df_90

# Load dataset
email_df2000 =pd.read_csv('/content/drive/MyDrive/Coursework/88-718/Data/email_df2000.csv')
email_df2000

email_df2000[email_df2000['From'].isna()]

print(email_df2000.iat[66397,3])

email_df2000.dropna(subset=['Body','From'], inplace=True)

email_df_2023 =pd.read_csv('/content/drive/MyDrive/Coursework/88-718/Data/email_df_2023.csv')
email_df_2023

email_df_2023[email_df_2023['From'].isna()]

"""## Merge all and save"""

frames = [email_df_90, email_df2000, email_df_2023]
allmails = pd.concat(frames)
allmails

allmails.dropna(subset=['Date'], inplace=True)

allmails

pattern = r"id AA02705sendmail 5.67/QC-subsidiary-2.1 via SMTPSun"
rows_with_pattern = allmails[allmails['Date'].str.contains(pattern)]

rows_with_pattern

allmails = allmails.drop(89497)

allmails

allmails.to_csv('allmails.csv',index=False)

"""# Removing duplicates"""

allmails_data = allmails

allmails

#duplicateRows = allmails_data[allmails_data.duplicated()]
duplicateRows = allmails_data[allmails_data.duplicated(['Date', 'Subject'])]
duplicateRows

duplicateCheck = allmails_data.duplicated(subset=['From','Date', 'Body','Subject'], keep=False)
allmails_data[duplicateCheck]

#duplicateCheck = dataSet.duplicated(subset=['Name', 'Date'], keep=False)
allmails_data_time = allmails_data.drop_duplicates(subset=['From','Date', 'Body','Subject'], keep='last')

"""### Fixing Dates

- different patterns and timezones

Patterns
Sun, 12 Mar 2023 22:02:33 -0400
Tue, 11 Sep 2001 17:33:19 +0200 (MET DST)
"""

allmails_data_time

import pandas as pd

# apply the split and extract logic to the 'failed_dates' column
#new_column = allmails_data_time['Date'].apply(lambda x: x.split('(', 1)[0].strip())

# add the new column to the dataframe
#allmails_data_time['just_date'] = new_column
# apply the split and extract logic to the 'failed_dates' column

# define a lambda function to apply different split and extract logics based on two conditions
def extract_date(x):
    if '(' in x:
        return x.split('(', 1)[0].strip()
    elif ',' in x:
        return x.split(',', 1)[1].strip()
    else:
        return x

# apply the lambda function to the column using the apply() method and store the result in a new column
allmails_data_time['just_date'] = allmails_data_time['Date'].apply(lambda x: extract_date(x))

# print the updated dataframe
allmails_data_time

allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0100', '01:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0200', '02:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0300', '03:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0400', '04:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0500', '05:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0600', '06:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0700', '07:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0800', '08:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0900', '09:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('1000', '10:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('1100', '11:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('1200', '12:00')

allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('edt', '-04:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('PDT', '-07:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('EST', '-04:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('PM', '-00:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('GMT', '-00:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('EDT', '-04:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'CET', '+01:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'PPE','-07:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'CDT','-05:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'MDT','-06:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'EAT','+03:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'PST','-08:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'MST','-07:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'UT','-00:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace( 'CST','-07:00')

allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('E. Australia Standard Time', '+10:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('GMT Standard Time', '-00:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('GMT','-00:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('GTB StandartSaati','-12:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('E. Europe Daylight Time', '+02:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('Pacific Standard Time', '-08:00')
allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('Eastern Standard Time', '-05:00')

allmails_data_time['just_date'] = allmails_data_time['just_date'].str.replace('0102', '2010')

# remove the space character at position 5 counting from the end of the string in the 'full_date' column, but only if the space character exists
allmails_data_time['just_date'] = allmails_data_time['just_date'].apply(lambda x: x[:-6] + x[-5:] if x[-6] == ' ' else x)

allmails_data_time['just_date']

import pandas as pd
from dateutil.parser import parse
import pendulum

# Function to convert date string to a standardized format
def standardize_date(date_str):
    try:
        dt = pendulum.parse(date_str, strict=False)
        return dt.format('YYYY-MM-DD HH:mm:ssZZ')
    except Exception as e:
        print(f"Failed to parse: {date_str}, Error: {e}")
        return None
# Standardize the date format for the 'Date' column
allmails_data_time['Date 1'] = allmails_data_time['just_date'].apply(lambda x: standardize_date(x) if isinstance(x, str) else x)

from datetime import datetime
from dateutil.parser import parse

failed_dates = []
parsed_dates = []

for date_string in allmails_data_time['just_date']:
    try:
        dt = datetime.strptime(date_string, "%Y-%m-%d")
        #print(dt)
    except ValueError:
        try:
            dt = parse(date_string)
            parsed_dates.append(dt)
            #print(dt)
        except ValueError:
            #print("Could not parse date:", date_string)
            failed_dates.append(date_string)

#print("Failed to parse:", failed_dates)

from datetime import datetime
from dateutil.parser import parse

parsed_dates = []
failed_dates = []

# iterate over the rows in the allmails_data_time DataFrame
for index, row in allmails_data_time.iterrows():
    date_string = row['just_date']
    try:
        dt = datetime.strptime(date_string, "%Y-%m-%d")
        parsed_dates.append(dt)
        failed_dates.append(None)
        #print(dt)
    except ValueError:
        try:
            dt = parse(date_string)
            parsed_dates.append(dt)
            failed_dates.append(None)
            #print(dt)
        except ValueError:
            #print("Could not parse date:", date_string)
            parsed_dates.append(None)
            failed_dates.append(date_string)

# create two new columns in the allmails_data_time DataFrame
allmails_data_time['parsed_dates'] = parsed_dates
allmails_data_time['failed_dates'] = failed_dates

# print the updated DataFrame
print(allmails_data_time.head())

# get the rows where the 'parsed_dates' column is null
mask = allmails_data_time['parsed_dates'].isnull()
filtered_df = allmails_data_time[mask]

# print the filtered DataFrame
print(filtered_df)

len(parsed_dates)

# drop the rows using the boolean mask
allmails_data_time.drop(allmails_data_time[mask].index, inplace=True)

# print the updated DataFrame
allmails_data_time

# drop the columns 'just_date', 'Date 1', and 'failed_dates'
#allmails_data_time.drop(['just_date', 'Date 1', 'failed_dates'], axis=1, inplace=True)

# print the updated DataFrame
allmails_data_time_parsed = allmails_data_time
allmails_data_time_parsed['parsed_dates']

# Convert the 'parsed_dates' column to pandas datetime dtype
allmails_data_time_parsed['parsed_dates_uct'] = pd.to_datetime(allmails_data_time_parsed['parsed_dates'], errors='coerce',utc=True)

# Check the dtype of the 'parsed_dates' column after conversion
allmails_data_time_parsed['parsed_dates_uct'].dtypes

import pandas as pd

# Find the rows with NaT values in the 'parsed_dates' column
nat_rows = allmails_data_time_parsed[pd.isna(allmails_data_time_parsed['parsed_dates_uct'])]

# Display the rows with NaT values
print(nat_rows)

# Drop the rows with NaT values in the 'parsed_dates' column
allmails_data_time_parsed = allmails_data_time_parsed.dropna(subset=['parsed_dates_uct'])

# Display the updated dataframe
print(allmails_data_time_parsed)

"""## saved"""

allmails_data_time_parsed_utc = allmails_data_time_parsed.drop(['just_date','Date 1','failed_dates'], axis=1)

allmails_data_time_parsed_utc

import pandas as pd
import datetime as dt
import pytz

# create a timezone object for UTC
tz_utc = pytz.timezone('UTC')

# create a tz-aware datetime object representing January 1, 1990
dt_1990 = pytz.utc.localize(pd.Timestamp('1990-01-01 00:00:00'))

# check if any dates are before 1990
if allmails_data_time_parsed_utc['parsed_dates_uct'].min() < dt_1990:
    print("Some dates are before 1990")
else:
    print("No dates are before 1990")

# select only the dates that are before 1990
dates_before_1990 = allmails_data_time_parsed_utc[
    allmails_data_time_parsed_utc['parsed_dates_uct'] < dt_1990
]['parsed_dates_uct']

# print the dates that are before 1990
print(dates_before_1990)

# create a timezone object for UTC
tz_utc = pytz.timezone('UTC')

# create a tz-aware datetime object representing January 1, 1990
dt_1990 = tz_utc.localize(pd.Timestamp('1990-01-01 00:00:00'))

# create a tz-aware datetime object representing April 1, 2023
dt_2023 = tz_utc.localize(pd.Timestamp('2023-04-01 00:00:00'))

# boolean indexing to select only the rows where the date is between 1990-01-01 and 2022-12-31
allmails_data_time_parsed_utc = allmails_data_time_parsed_utc[
    (allmails_data_time_parsed_utc['parsed_dates_uct'] >= dt_1990) &
    (allmails_data_time_parsed_utc['parsed_dates_uct'] <= dt_2023)
]
# print the updated DataFrame
print(allmails_data_time_parsed_utc)
# print the updated Series
allmails_data_time_parsed_utc['parsed_dates_uct']

allmails_data_time_parsed_utc

allmails_data_time_parsed_utc.to_csv('allmails_data_time_parsed_utc.csv', index=False)